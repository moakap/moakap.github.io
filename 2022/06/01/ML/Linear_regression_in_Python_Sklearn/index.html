<!DOCTYPE html>
<html lang="zh-CN,en,default">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/snail-filled.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/snail-filled-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/snail-filled-16x16.png">
  <link rel="mask-icon" href="/images/snail-filled.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"moakap.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="这里重点学习一下回归算法 机器学习基础机器学习 vs. 传统编程 三种类别根据使用场景不同，有可以划分为三大类 监督学习监督学习是根据已知的方法和规律，对新样本进行分类和预测。监督学习就好比学生在学校学习各种知识和理论的过程。整个过程是在老师的监督下完成，所学知识的应用体现在平时的测试中，并且可以通过对比“标准”答案来不断改进和巩固学习成果，在下一次的测试中取得更好的成绩（预测效果）。根据目的不">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习之Python Sklearn——线性回归">
<meta property="og:url" content="https://moakap.github.io/2022/06/01/ML/Linear_regression_in_Python_Sklearn/index.html">
<meta property="og:site_name" content="Moakap&#39;s Blog">
<meta property="og:description" content="这里重点学习一下回归算法 机器学习基础机器学习 vs. 传统编程 三种类别根据使用场景不同，有可以划分为三大类 监督学习监督学习是根据已知的方法和规律，对新样本进行分类和预测。监督学习就好比学生在学校学习各种知识和理论的过程。整个过程是在老师的监督下完成，所学知识的应用体现在平时的测试中，并且可以通过对比“标准”答案来不断改进和巩固学习成果，在下一次的测试中取得更好的成绩（预测效果）。根据目的不">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://github.com/moakap/moakap.github.io/assets/6308127/8778ba9b-b307-483e-a819-bbdcb3caa0e6">
<meta property="og:image" content="https://user-images.githubusercontent.com/6308127/158024257-4a667037-e685-4875-98a0-71fbf52549c5.png">
<meta property="og:image" content="https://github.com/moakap/moakap.github.io/assets/6308127/9f65e409-5b3e-4fb6-8562-d265e1f8529b">
<meta property="og:image" content="https://user-images.githubusercontent.com/6308127/158024353-55d53328-ecd2-4fae-a888-5eb1ed0ac771.png">
<meta property="og:image" content="https://github.com/moakap/moakap.github.io/assets/6308127/be0eb8c9-5ad5-4833-9f25-bacc8ac7d8be">
<meta property="og:image" content="https://github.com/moakap/moakap.github.io/assets/6308127/2dabe469-f961-4042-8aa9-d0be2c69c237">
<meta property="og:image" content="https://github.com/moakap/moakap.github.io/assets/6308127/d00ad045-6ed6-4e03-aa25-71789c15485b">
<meta property="og:image" content="https://github.com/moakap/moakap.github.io/assets/6308127/4243d0aa-dd7a-4149-bf64-0e09a359c0ed">
<meta property="og:image" content="https://github.com/moakap/moakap.github.io/assets/6308127/8928205c-4bab-423e-b754-27eb7e510c10">
<meta property="og:image" content="https://github.com/moakap/moakap.github.io/assets/6308127/e4162e96-5b35-406a-bfc2-f0644fe42a4d">
<meta property="og:image" content="https://github.com/moakap/moakap.github.io/assets/6308127/32e0ee89-64ec-4805-9660-4374e5b13ff4">
<meta property="og:image" content="https://github.com/moakap/moakap.github.io/assets/6308127/e222c2dc-f8bc-4df3-8c71-8ec62cd9452f">
<meta property="article:published_time" content="2022-06-01T07:15:51.000Z">
<meta property="article:modified_time" content="2023-07-03T03:08:44.317Z">
<meta property="article:author" content="moakap">
<meta property="article:tag" content="ML">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Linear Regression">
<meta property="article:tag" content="Sklearn">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/moakap/moakap.github.io/assets/6308127/8778ba9b-b307-483e-a819-bbdcb3caa0e6">

<link rel="canonical" href="https://moakap.github.io/2022/06/01/ML/Linear_regression_in_Python_Sklearn/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>机器学习之Python Sklearn——线性回归 | Moakap's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-DFB6WMHQPG"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-DFB6WMHQPG');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Moakap's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Keep moving, keep thinking!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">32</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">10</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">20</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://moakap.github.io/2022/06/01/ML/Linear_regression_in_Python_Sklearn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/snail-filled.png">
      <meta itemprop="name" content="moakap">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Moakap's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习之Python Sklearn——线性回归
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-06-01 15:15:51" itemprop="dateCreated datePublished" datetime="2022-06-01T15:15:51+08:00">2022-06-01</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%9C%BB%E8%9C%93%E7%82%B9%E6%B0%B4/" itemprop="url" rel="index"><span itemprop="name">蜻蜓点水</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%9C%BB%E8%9C%93%E7%82%B9%E6%B0%B4/ml/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%9C%BB%E8%9C%93%E7%82%B9%E6%B0%B4/ml/sklearn/" itemprop="url" rel="index"><span itemprop="name">Sklearn</span></a>
                </span>
            </span>

          
            <span id="/2022/06/01/ML/Linear_regression_in_Python_Sklearn/" class="post-meta-item leancloud_visitors" data-flag-title="机器学习之Python Sklearn——线性回归" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/06/01/ML/Linear_regression_in_Python_Sklearn/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/06/01/ML/Linear_regression_in_Python_Sklearn/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="https://github.com/moakap/moakap.github.io/assets/6308127/8778ba9b-b307-483e-a819-bbdcb3caa0e6" alt="image"></p>
<p>这里重点学习一下回归算法</p>
<h2 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h2><h3 id="机器学习-vs-传统编程"><a href="#机器学习-vs-传统编程" class="headerlink" title="机器学习 vs. 传统编程"></a>机器学习 vs. 传统编程</h3><p><img src="https://user-images.githubusercontent.com/6308127/158024257-4a667037-e685-4875-98a0-71fbf52549c5.png" alt="image"></p>
<h3 id="三种类别"><a href="#三种类别" class="headerlink" title="三种类别"></a>三种类别</h3><p>根据使用场景不同，有可以划分为三大类</p>
<h4 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h4><p>监督学习是根据已知的方法和规律，对新样本进行分类和预测。<br>监督学习就好比学生在学校学习各种知识和理论的过程。整个过程是在老师的监督下完成，所学知识的应用体现在平时的测试中，并且可以通过对比“标准”答案来不断改进和巩固学习成果，在下一次的测试中取得更好的成绩（预测效果）。根据目的不同，可以有分类和回归（预测）两种：</p>
<ul>
<li>分类 —— 根据事物特征将其划分到已知的类别。<ul>
<li>根据事物的特征，将新遇到的事物（新样本）归类到已知的某个类别。</li>
</ul>
</li>
<li>回归（预测）—— 根据事物的特征和预测将来的发展趋势。<ul>
<li>根据事物的特征和发展规律，预测将来的发展趋势。</li>
</ul>
</li>
</ul>
<span id="more"></span>

<h4 id="非监督学习"><a href="#非监督学习" class="headerlink" title="非监督学习"></a>非监督学习</h4><p>非监督学习与监督学习正好相反，在过程中并不清楚有哪些特征标签，而是通过对样本的观察、分析和总结，去发现其中的特征，从看似杂乱无章的数据中发现共性。然后对新样本进行归类。</p>
<blockquote>
<p>归类/聚类（clustering）—— 对大量看似无特征的样本进行分类。</p>
</blockquote>
<p>经常会说非监督学习是归类或聚类（clustering）。与分类不同，聚类在划分之前并不知道有哪些类别，以及类别的特征，而是通过对样本的各种特征进行分析，将相似的样本划分为一组，进行“物以类聚”的划分。是一个从无到有的发现过程。</p>
<h4 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h4><p>介于监督和非监督学习之间。是基于监督学习获得理论和知识，对非监督学习的样本（无特征标签）进行分类。可以理解为我们走出学校以后根据在课堂上学到的知识和理论，对生活中遇到的事物进行分类和预测。</p>
<h2 id="Python-sklearn-scikit-learn"><a href="#Python-sklearn-scikit-learn" class="headerlink" title="Python sklearn (scikit-learn)"></a>Python sklearn (scikit-learn)</h2><p>Scikit-learn是一个用于Python的免费开源机器学习库。</p>
<h3 id="机器学习三大步骤"><a href="#机器学习三大步骤" class="headerlink" title="机器学习三大步骤"></a>机器学习三大步骤</h3><p>机器学习的本质就是让机器使用特定的算法对输入数据进行类似人的智能学习（找规律），根据同样的模型对新样本进行进行预测。具体到python中的sklearn，是通过一下三大步骤实现的。</p>
<p><img src="https://github.com/moakap/moakap.github.io/assets/6308127/9f65e409-5b3e-4fb6-8562-d265e1f8529b" alt="image"></p>
<ol>
<li>准备数据——对输入数据进行预处理</li>
<li>选择、训练和测试模型——使用处理后的样本数据，针对特定模型进行拟合、训练<ol>
<li>模型的选择，可以根据要解决的问题和使用场景，scikit-learn提供了一张图（<a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html">选择正确的估计器</a>）来详细描述模型选择的流程，最终指向机器学习的集中主要使用场景，<strong>回归、分类</strong>和<strong>聚类</strong>，以及scikit-learn提供的<strong>数据降维</strong>方法。</li>
</ol>
</li>
</ol>
<p><img src="https://user-images.githubusercontent.com/6308127/158024353-55d53328-ecd2-4fae-a888-5eb1ed0ac771.png" alt="image"></p>
<ol start="3">
<li>使用模型预测——使用训练后的模型对新样本进行预测，并根据预测结果过进一步优化</li>
</ol>
<h3 id="scikit-learn-sklearn-实现"><a href="#scikit-learn-sklearn-实现" class="headerlink" title="scikit-learn (sklearn) 实现"></a>scikit-learn (sklearn) 实现</h3><p>sklearn中内置了常用的机器学习算法和模型，以及基本的预处理方法。分别由<strong>预测（估计）器（estimator）</strong>和<strong>预处理器（preprocessing）</strong>实现，并且它们继承自同一个基础预测器。</p>
<p><img src="https://github.com/moakap/moakap.github.io/assets/6308127/be0eb8c9-5ad5-4833-9f25-bacc8ac7d8be" alt="image"></p>
<h4 id="转换器和预处理器"><a href="#转换器和预处理器" class="headerlink" title="转换器和预处理器"></a>转换器和预处理器</h4><p>预处理器和转换器主要负责对原始数据的预处理和转换，从而消除不同数据之间的绝对差异。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="comment"># X = [...]</span></span><br><span class="line"></span><br><span class="line">StandardScaler().fit().transform(X)</span><br></pre></td></tr></table></figure>

<h4 id="预测器"><a href="#预测器" class="headerlink" title="预测器"></a>预测器</h4><p>sklearn中提供的数十种内置机器学习算法和模型，都通过估算器提供。可以通过引入相应的估算器来使用对应的模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化估算器</span></span><br><span class="line">clf = RandomForestClassifier(random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用训练数据进行模型拟合</span></span><br><span class="line">clf.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">clf.predict(...) <span class="comment"># </span></span><br></pre></td></tr></table></figure>

<h4 id="管道"><a href="#管道" class="headerlink" title="管道"></a>管道</h4><p>管道用来将转换器和预测器组合成一个同一的对象，表示一个完整的数据流处理过程（管道）。然后使用管道的fit和predict来进行训练和预测。同时，使用管道还可以防止数据泄露。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler <span class="comment"># 预处理器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="comment"># 预测器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline <span class="comment"># 管道</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建管道</span></span><br><span class="line">pipe = make_pipeline(</span><br><span class="line">    StandardScaler(),</span><br><span class="line">    LogisticRegression(random_state=<span class="number">0</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#准备数据...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 像使用预测器一样使用管道</span></span><br><span class="line"><span class="comment"># 1) 训练整个pipeline</span></span><br><span class="line">pipe.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2) 使用管道预测</span></span><br><span class="line">pipe.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>针对不同类型的算法和模型，对应评估指标也不相同。</p>
<h4 id="回归算法指标"><a href="#回归算法指标" class="headerlink" title="回归算法指标"></a>回归算法指标</h4><p>回归算法的核心思想是评估预测值和观察值之间的误差。从最原始的残差（residual）到基于残差的各种变形，以便后续的数学运算和处理。</p>
<ul>
<li>Mean Absolute Error 平均绝对误差：对残差做绝对值处理，避免残差正负导致的相互抵消。</li>
<li>Mean Squared Error 均方误差：为了便于求导，对平均绝对误差进行平方。</li>
<li>Root Mean Squared Error 均方根误差：如果目标变量的量纲保持一致，可以对均方误差进行开放。</li>
<li>Coefficient of determination 决定系数：进一步去除对量纲的依赖。</li>
</ul>
<h4 id="分类算法指标"><a href="#分类算法指标" class="headerlink" title="分类算法指标"></a>分类算法指标</h4><p>相比较回归算法的各种残差指标，分类算法更多的则是关注分类的精度，即预测正确的样本数量占总预测样本的比例。然后，预计不同的场景，从不同角度来看精度对结果的影响。</p>
<ul>
<li>Accuracy 精度：每一个分类中预测正确的样本数占总样本的比例。</li>
<li>混淆矩阵 Confusion Matrix</li>
<li>准确率（查准率） Precision</li>
<li>召回率（查全率）Recall</li>
<li>Fβ Score</li>
<li>AUC Area Under Curve</li>
<li>KS Kolmogorov-Smirnov</li>
</ul>
<p>关于各个指标的详细说明，可以参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/36305931">知乎——机器学习评估指标</a>。</p>
<h3 id="参数搜索"><a href="#参数搜索" class="headerlink" title="参数搜索"></a>参数搜索</h3><p>所有预测器都有可以调整的参数，也叫<strong>超参数</strong>。其特指不能通过学习得到的参数。在使用各种不同模型时，需要将超参数作为参数传递给预测器。<br>sklearn中提供了在参数集中搜索最佳超参数的方法，其基于最佳<a target="_blank" rel="noopener" href="http://scikit-learn.org.cn/view/6.html"><strong>交叉验证</strong></a><strong>（CV, Cross-Validation）分数</strong>获取对应的超参数。sklearn中最简单的方法是调用<a target="_blank" rel="noopener" href="https://scikit-learn.org.cn/view/663.html">cross_val_score</a>函数。</p>
<blockquote>
<p>估算器中的任意参数都可以通过参数搜索来获得最佳参数。</p>
</blockquote>
<p>参数搜索的基本要素</p>
<ul>
<li>参数搜索方法</li>
<li>评估方法</li>
</ul>
<p>sklearn中两种抽样搜索最佳参数的方法：</p>
<h4 id="GridSearchCV-网格搜索方法"><a href="#GridSearchCV-网格搜索方法" class="headerlink" title="GridSearchCV 网格搜索方法"></a>GridSearchCV 网格搜索方法</h4><p>计算参数集中所有参数的组合。GridSearchCV通过使用param_grid参数来指定参数候选值。<br>例如下边的例子指定搜索两个参数候选值。通过GridSearchCV的fitting接口拟合后，会对所有候选参数进行评估，保留最优参数组合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选择模型</span></span><br><span class="line">clf = ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定网格搜索的参数</span></span><br><span class="line">param_grid = [</span><br><span class="line">  &#123;<span class="string">&#x27;C&#x27;</span>: [<span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>], <span class="string">&#x27;kernel&#x27;</span>: [<span class="string">&#x27;linear&#x27;</span>]&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;C&#x27;</span>: [<span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>], <span class="string">&#x27;gamma&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.0001</span>], <span class="string">&#x27;kernel&#x27;</span>: [<span class="string">&#x27;rbf&#x27;</span>]&#125;,</span><br><span class="line"> ] </span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行网格搜索</span></span><br><span class="line">grid_search = GridSearchCV(clf, param_grid=param_grid)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">grid_search.fit(X, y)</span><br></pre></td></tr></table></figure>
<h4 id="RandomizedSearchCV-随机搜索方法"><a href="#RandomizedSearchCV-随机搜索方法" class="headerlink" title="RandomizedSearchCV 随机搜索方法"></a>RandomizedSearchCV 随机搜索方法</h4><p>从具有制定分布的参数空间中抽样出定量的参数候选。RandomizedSearchCV会在指定范围内，使用某个特定分布抽取参数值进行评估。用字典形式来指定参数的具体抽样范围，针对每一个参数，可以指定</p>
<ul>
<li>具体的参数</li>
<li>采样分布——可以使用scipy.stats模块，其中包含了很多采样分布，如expon，gamma，uniform或者randint。</li>
<li>离散选项列表</li>
</ul>
<p>例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选择模型</span></span><br><span class="line">clf = ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># RandomizedSearchCV 参数设置示例</span></span><br><span class="line">param_dist = &#123;</span><br><span class="line">    <span class="string">&#x27;C&#x27;</span>: scipy.stats.expon(scale=<span class="number">100</span>),  <span class="comment"># 取值分布</span></span><br><span class="line">    <span class="string">&#x27;gamma&#x27;</span>: scipy.stats.expon(scale=<span class="number">.1</span>),</span><br><span class="line">    <span class="string">&#x27;kernel&#x27;</span>: [<span class="string">&#x27;rbf&#x27;</span>], <span class="comment"># 离散列表</span></span><br><span class="line">    <span class="string">&#x27;class_weight&#x27;</span>:[<span class="string">&#x27;balanced&#x27;</span>, <span class="literal">None</span>] <span class="comment"># 离散列表</span></span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行随机搜索</span></span><br><span class="line">n_iter_search = <span class="number">20</span></span><br><span class="line">random_search = RandomizedSearchCV(clf, param_distributions=param_dist,</span><br><span class="line">                                   n_iter=n_iter_search)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">random_search.fit(X, y)</span><br></pre></td></tr></table></figure>
<h4 id="评估方法（指标）"><a href="#评估方法（指标）" class="headerlink" title="评估方法（指标）"></a>评估方法（指标）</h4><p>参数搜索默认使用估计器的score函数来评估参数的设置。默认为</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://scikit-learn.org.cn/view/476.html">sklearn.metrics.accuracy_score</a> 用于分类</li>
<li><a target="_blank" rel="noopener" href="https://scikit-learn.org.cn/view/519.html">sklearn.metrics.r2_score</a> 应用于回归</li>
</ul>
<p>当然，sklearn还提供了其它一些<a target="_blank" rel="noopener" href="http://scikit-learn.org.cn/view/93.html#3.3.1%20%E8%AF%84%E5%88%86%E5%8F%82%E6%95%B0%EF%BC%9A%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E5%87%86%E5%88%99">评估函数</a>可供不从场景使用。同时，也可以将多种评估指标结合起来使用。</p>
<h2 id="回归算法"><a href="#回归算法" class="headerlink" title="回归算法"></a>回归算法</h2><p>从sklearn模型流程可以很清楚得看到回归算法的使用场景。</p>
<p><img src="https://github.com/moakap/moakap.github.io/assets/6308127/2dabe469-f961-4042-8aa9-d0be2c69c237" alt="image"></p>
<p>使用场景</p>
<ul>
<li>样本数大于50</li>
<li>需要预测将来的某个具体数据（而不是对数据分类）</li>
</ul>
<p>然后根据样本数据数据和特征标签的特点选择对应的算法</p>
<ul>
<li>样本数大于100K时，推荐使用SGDRegressor</li>
<li>样本数小于100K时，则推荐使用Ridge, Lasso, 或者ElasticNet等；然后根据样本数据特征标签的权重，进行进一步选择。<ul>
<li>如果样本数据的特征标签同等重要，可以选择RidgeRegression或SVR;</li>
<li>如果样本数据的某些特征更重要，对结果影响更大，可以使用Lasso和ElasticNet；<h3 id="回归分析（Regression-Analysis）"><a href="#回归分析（Regression-Analysis）" class="headerlink" title="回归分析（Regression Analysis）"></a>回归分析（Regression Analysis）</h3>在搞清楚线性回归之前，我们首先要弄明白什么是<strong>回归分析</strong>。在统计学中，是指<strong>确定两种或两种以上变量间相互依赖的定量关系</strong>的一种统计分析方法。<br>按照涉及的变量多少，可以分为<strong>一元回归</strong>和<strong>多元回归</strong>；按照因变量的多少，可分为<strong>简单回归分析</strong>和<strong>多重回归分析</strong>；按照自变量和因变量之间的关系类型，可分为<strong>线性回归分析</strong>和<strong>非线性回归分析</strong>。</li>
</ul>
</li>
</ul>
<h3 id="线性回归（Liner-Regression）基础"><a href="#线性回归（Liner-Regression）基础" class="headerlink" title="线性回归（Liner Regression）基础"></a>线性回归（Liner Regression）基础</h3><p>线性回归假设目标值（因变量）是特征值（自变量）的线性组合。线性回归使用最佳的拟合直线在因变量（$Y$）和一个或多个自变量（$X$）之间建立一种关系。因变量是连续的，自变量可以是连续的也可以是离散的，回归线的性质是线性的。<br>在数学中，即为由特征值组成的多元一次方程。<br>$Y=b+w_1x_1+…+w_px_p+\epsilon$<br>如果将自变量表示为$X=(x_1,…x_p)$，系数表示为$W=(w_1, …, w_p)$，就能明白为什么它叫线性回归了。<br>$Y=b+W^TX+\epsilon$<br>其中$W$是回归线的斜率，$w_0$则表示回归线在Y轴上的截距，$\epsilon$表示误差项。<br>上边的表示中，我们默认自变量和因变量都是多元的。这里我们限制因变量的个数为一个，则可以直接表示为<br>$y=b+W^TX+\epsilon$<br>线性回归就是通过各种算法去找到这样一个多元一次方程$\hat{y}(x)=b+wX$，使其尽量接近观真实值$y$（残差$\epsilon$尽量小）。而拟合的过程，就是去不断优化和估计方程的斜率系数，所以也叫“回归系数（coefficient）”。<br>这里要注意模型<strong>偏差bias</strong>和<strong>残差（噪声）</strong>的区别。</p>
<ul>
<li>偏差$b$——指排除噪声影响下，预测结果与真实值之间的差异。其主要是由模型的拟合度不够导致的（不是随机的，并且可通过一定的特征工程进行预测）。</li>
<li>残差$\epsilon$——预测结果与真实值之间的差异。在模型完全拟合的情况下，与真实值之间的差异，因此可以理解为噪声（随机、不可预测的）。</li>
</ul>
<h4 id="预测值和真实值的距离"><a href="#预测值和真实值的距离" class="headerlink" title="预测值和真实值的距离"></a>预测值和真实值的距离</h4><p>那么，如何衡量预测的准确性，即衡量预测值$\hat{y}(x)$与真实值$y$之间的差异呢？最直观的方法就是看真实值和预测值之间的差(残差)。回归模型的目的就是使模型预测出来的值无限接近真实值（测量值）。<br>$\hat{\epsilon} = y - \hat{y}(x)$<br>但是在实际预测中，残差值有正有负，不可能直接使用残差之和最小的方式来衡量一种方法的好坏。<br>因此，整个回归问题的本质，就是使用均方误差，求使$D$最小时的$W$和$d$。<br>$D=E(y-\hat{y}(x))^2$</p>
<h5 id="1-残差平方和SSE"><a href="#1-残差平方和SSE" class="headerlink" title="1. 残差平方和SSE"></a>1. 残差平方和SSE</h5><p>由于残差本身有正有负，故可以使用平方和来避免正负抵消问题。<br>$dist(P_i, P_j) = \sum_{k=1}^n(P_ik-P_jk)^2$<br>问题：</p>
<ol>
<li><p>使用平方后会放大（差&gt;1）部分的误差，同时缩小（-1&lt;差&lt;1）部分的误差；</p>
</li>
<li><p>当不同维度的度量差异很大时无法处理；</p>
<h5 id="2-欧氏距离"><a href="#2-欧氏距离" class="headerlink" title="2. 欧氏距离"></a>2. 欧氏距离</h5><p>为了解决误差平方和的问题，我们可以使用欧式距离。<br>$dist(P_i, P_j) = \sqrt{\sum_{k=1}^n(P_ik-P_jk)^2}$<br>问题：</p>
</li>
<li><p>求解麻烦；</p>
</li>
<li><p>不同问题的度量差异很大时无法处理；</p>
<h5 id="3-曼哈顿距离"><a href="#3-曼哈顿距离" class="headerlink" title="3. 曼哈顿距离"></a>3. 曼哈顿距离</h5><p>曼哈顿距离直接使用绝对值来消除根号开方的求解麻烦问题。<br>$dist(P_i, P_j) = \sum_{k=1}^n\vert P_ik-P_jk\vert$<br>问题：</p>
</li>
<li><p>不是连续函数，<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%B1%82%E5%AF%BC/1063861">求导</a>很麻烦，计算不方便，只能计算垂直、水平距离</p>
</li>
</ol>
<p>适合场景：数据稀疏（自带归一化处理）</p>
<h5 id="4-马氏距离-Mahalanobis-Distance"><a href="#4-马氏距离-Mahalanobis-Distance" class="headerlink" title="4. 马氏距离(Mahalanobis Distance)"></a>4. 马氏距离(Mahalanobis Distance)</h5><p>马氏距离是对欧式距离的另外一种修正，修正了欧氏距离中各个维度尺度不一致且相关的问题。<br>$dist(P_i, P_j) = \sqrt{(P_ik-P_jk)^T\Sigma^{-1}(P_ik-P_jk)}$<br>马氏距离已经不像前边的几种那么好理解，具体推导过程可以参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46626607">马氏距离</a>。总之，其较好的解决了不同维度尺度不一致且相关的问题。</p>
<h5 id="5-其它"><a href="#5-其它" class="headerlink" title="5. 其它"></a>5. 其它</h5><p>其它还有一些距离如汉明距离(Hamming Distance)、编辑距离(Levenshtein Distance)等，这里不做一一说明。</p>
<h4 id="普通最小二乘法-Ordinary-Least-Square-OLS"><a href="#普通最小二乘法-Ordinary-Least-Square-OLS" class="headerlink" title="普通最小二乘法(Ordinary Least Square, OLS)"></a>普通最小二乘法(Ordinary Least Square, OLS)</h4><p>最小二乘法直接使用残差的平方和作为衡量标准，通过拟合$W$使残差$\epsilon$的平方和最小。数学上表示为下边的形式：<br>$min{\Vert Xw-y\Vert _2^2}$</p>
<p><img src="https://github.com/moakap/moakap.github.io/assets/6308127/d00ad045-6ed6-4e03-aa25-71789c15485b" alt="image"></p>
<p>在sklearn的线性回归模型LinearRegression中，使用$fit()$函数拟合模型，并在模型的coef_中存储拟合后的相关系数$w$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets, linear_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...准备数据，选择特征列，拆分训练、测试数据集..</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create linear regression object</span></span><br><span class="line">regr = linear_model.LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model using the training sets</span></span><br><span class="line">regr.fit(diabetes_X_train, diabetes_y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions using the testing set</span></span><br><span class="line">diabetes_y_pred = regr.predict(diabetes_X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The coefficients</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Coefficients: \n&quot;</span>, regr.coef_)</span><br><span class="line"><span class="comment"># The mean squared error</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Mean squared error: %.2f&quot;</span> % mean_squared_error(diabetes_y_test, diabetes_y_pred))</span><br><span class="line"><span class="comment"># The coefficient of determination: 1 is perfect prediction</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Coefficient of determination: %.2f&quot;</span> % r2_score(diabetes_y_test, diabetes_y_pred))</span><br></pre></td></tr></table></figure>
<h5 id="非负最小方差"><a href="#非负最小方差" class="headerlink" title="非负最小方差"></a>非负最小方差</h5><p>在最小二乘法法返回的系数中，默认是不会对系数的正负进行限制的。但是在实际问题中，很多时候我们需要所有的相关系数非负，例如频次、商品价格等。这时候可以直接设置LinearRegression的positive参数为True来限制相关系数的正负。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">reg_nnls = LinearRegression(positive=<span class="literal">True</span>)</span><br><span class="line">y_pred_nnls = reg_nnls.fit(X_train, y_train).predict(X_test)</span><br><span class="line">r2_score_nnls = r2_score(y_test, y_pred_nnls)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;NNLS R2 score&quot;</span>, r2_score_nnls)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>问题<br>普通最小二乘的系数估计依赖于特征的独立性。当特征相关且设计矩阵的列之间具有近似线性相关性时， 设计矩阵趋于奇异矩阵，最小二乘估计对观测目标的随机误差高度敏感，可能产生很大的方差。例如，在没有实验设计的情况下收集数据时，就可能会出现这种多重共线性的情况。</p>
</blockquote>
<h4 id="过拟合-Overfitting-和欠拟合-Underfitting"><a href="#过拟合-Overfitting-和欠拟合-Underfitting" class="headerlink" title="过拟合(Overfitting)和欠拟合(Underfitting)"></a>过拟合(Overfitting)和欠拟合(Underfitting)</h4><p>过拟合和欠拟合是使用实际数据进行分析时可能会遇到的两种基本问题。</p>
<p><img src="https://github.com/moakap/moakap.github.io/assets/6308127/4243d0aa-dd7a-4149-bf64-0e09a359c0ed" alt="image"></p>
<ul>
<li><strong>欠拟合</strong>——指模型在训练集中的表现就很差了，经验误差很大。</li>
</ul>
<p>欠拟合出现的原因是模型复杂度太低，比如自变量太少等。针对欠拟合，要做的就是增大模型复杂度，增加自变量，或者改变模型（线性到非线性）等。</p>
<ul>
<li><strong>过拟合</strong>——指模型在训练集中表现良好，而测试集中表现很差，即泛化误差大于了经验误差，拟合过度，模型泛化能力降低，只能够适用于训练集，通用性不强。</li>
</ul>
<p>过拟合的原因是模型复杂度太高，或者训练数据集太少，比如自变量过多等。针对过拟合，除了增加训练集数据外，正则化就是常用的一种处理方法。</p>
<h5 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h5><ul>
<li><strong>正则化(Regularization)</strong> 是机器学习中对原始损失函数引入额外信息，以便防止过拟合和提高模型泛化性能的一类方法的统称。也就是目标函数变成了<strong>原始损失函数+额外项</strong>，常用的额外项一般有两种，英文称作ℓ1−𝑛𝑜𝑟𝑚ℓ1−norm和ℓ2−𝑛𝑜𝑟𝑚ℓ2−norm，中文称作<strong>L1正则化</strong>和<strong>L2正则化</strong>，或者L1范数和L2范数（实际是L2范数的平方）。</li>
<li>L1正则化和L2正则化可以看做是<strong>损失函数的惩罚项</strong>。所谓<strong>惩罚</strong>是指对损失函数中的<strong>某些参数做一些限制</strong>。对于线性回归模型，<strong>使用L1正则化的模型叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）</strong>。</li>
</ul>
<p>关于正则化的更多详细理解可以参考<a target="_blank" rel="noopener" href="https://www.cnblogs.com/zingp/p/10375691.html#_label0">深入理解L1、L2正则化</a>，以及<a target="_blank" rel="noopener" href="https://blog.csdn.net/jinping_shi/article/details/52433975">机器学习中正则化项L1和L2的直观理解</a>。</p>
<h4 id="岭-Ridge-回归（L2正则化）"><a href="#岭-Ridge-回归（L2正则化）" class="headerlink" title="岭(Ridge)回归（L2正则化）"></a>岭(Ridge)回归（L2正则化）</h4><p>针对最小二乘法存在的一下问题，岭回归则计算一个<strong>带惩罚项的残差平方和</strong>。<br>$min{\Vert Xw-y\Vert _2^2}+\alpha\Vert w \Vert _2^2$<br>其中$\Vert w \Vert _2^2$为回归系数向量的L2范数（所有参数的平方和）。使用复杂度参数$\alpha \ge0$来控制收缩程度：值越大，收缩程度越大，对应的回归系数对共线性的容忍程度也就越大。至于为什么L2正则化能防止过拟合，可以参考<a target="_blank" rel="noopener" href="https://www.cnblogs.com/zingp/p/10375691.html#_label0">深入理解L1、L2正则化</a>。</p>
<p><img src="https://github.com/moakap/moakap.github.io/assets/6308127/8928205c-4bab-423e-b754-27eb7e510c10" alt="image"></p>
<p>在sklearn中，可以通过指定alpha参数来设定$\alpha$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定alpha的岭回归</span></span><br><span class="line">reg = linear_model.Ridge(alpha=<span class="number">.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合</span></span><br><span class="line">reg.fit([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]], [<span class="number">0</span>, <span class="number">.1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<h4 id="Lasso回归（L1正则化）"><a href="#Lasso回归（L1正则化）" class="headerlink" title="Lasso回归（L1正则化）"></a>Lasso回归（L1正则化）</h4><p>Lasso回归与Ridge岭回归类似，都是在普通最小二乘法基础上的正则化处理。目标函数在数学上表示为<br>$min{\frac1{2n_{sample}}\Vert Xw-y\Vert _2^2}+\alpha\Vert w \Vert_1$<br>其中$\Vert w \Vert _1$为回归系数向量的L1范数（所有参数绝对值之和）。</p>
<h5 id="特征选取与压缩感知"><a href="#特征选取与压缩感知" class="headerlink" title="特征选取与压缩感知"></a>特征选取与压缩感知</h5><p>特征选取就是对样本进行稀疏表示的过程。而Lasso正好就是估计稀疏系数的线性模型，因为它倾向于给出非零系数较少的解，从而有效地减少了给定解所依赖的特征数。至于为什么L1正则化能减少给定解所依赖的特征数，可以参考<a target="_blank" rel="noopener" href="https://www.cnblogs.com/zingp/p/10375691.html#_label0">深入理解L1、L2正则化</a>。<br>所以说，Lasso 及其变体是压缩感知领域的基础。关于稀疏表示，参考 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/151901026">机器学习基础——稀疏表示</a>。</p>
<blockquote>
<p>稀疏表示<br>任意一个信号都可以在一个过完备字典上稀疏线性表出。这样，一个信号被分解为有限个信号的线形组合的形式我们称之为稀疏表示。表达为公式为：<br>y = D_α_ <em>s.t</em>.||α||0 &lt; σ</p>
</blockquote>
<h4 id="Elastic-Net弹性网络回归（L1-L2正则化）"><a href="#Elastic-Net弹性网络回归（L1-L2正则化）" class="headerlink" title="Elastic-Net弹性网络回归（L1+L2正则化）"></a>Elastic-Net弹性网络回归（L1+L2正则化）</h4><p>弹性网络回归是一个综合了Ridge回归和Lasso回归两种惩罚因数的单一模型：一个因素与L1范数成比例，另外一个因数与L2范数成比例。其目标函数表示为<br>$min{\frac1{2n_{sample}}\Vert Xw-y\Vert _2^2}+\alpha\beta\Vert w \Vert_1+\frac{\alpha(1-\beta)}{2} \Vert w \Vert_2^2$<br>从上边的公式可以看出，ElasticNet使用时需要提供$\alpha$和$\beta$两个参数。其中$\beta$的参数名称为l1_ratio。</p>
<blockquote>
<p>当多个特征存在相关时，弹性网是很有用的。Lasso很可能随机挑选其中之一，而弹性网则可能兼而有之。</p>
</blockquote>
<h4 id="预估正则化参数——贝叶斯回归"><a href="#预估正则化参数——贝叶斯回归" class="headerlink" title="预估正则化参数——贝叶斯回归"></a>预估正则化参数——贝叶斯回归</h4><p>主要用于预估正则化参数：正则化参数不是应意义上的设置，而是根据数据进行调整。</p>
<h5 id="贝叶斯岭回归"><a href="#贝叶斯岭回归" class="headerlink" title="贝叶斯岭回归"></a>贝叶斯岭回归</h5><h5 id="自动关联判定-ARD"><a href="#自动关联判定-ARD" class="headerlink" title="自动关联判定-ARD"></a>自动关联判定-ARD</h5><h4 id="高维数据的线性回归"><a href="#高维数据的线性回归" class="headerlink" title="高维数据的线性回归"></a>高维数据的线性回归</h4><h5 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h5><p>在实际应用中，特征数量往往非常多，其中即包含了我们需要的与目标相关的特征，也有一些完全不相关的特征，并且特征之间也可能存在相互依赖。这会导致对应的模型就越复杂，模型训练和预测需要的计算量就越大，同时也会影响算法的预测能力。<br>特征选取就是从大量的特征中选取一个特征子集，构造出更好的模型（如残差最小）。<br>特征选择分为产生、评估、验证三大步骤，如下图。</p>
<p><img src="https://github.com/moakap/moakap.github.io/assets/6308127/e4162e96-5b35-406a-bfc2-f0644fe42a4d" alt="image"></p>
<p>特征选择的过程 ( M. Dash and H. Liu 1997 )</p>
<h6 id="1-特征生成"><a href="#1-特征生成" class="headerlink" title="1. 特征生成"></a>1. 特征生成</h6><p>特征产生过程是搜索特征子空间的过程。分为一下3大类</p>
<ul>
<li>完全搜索</li>
</ul>
<p>完全搜索又分为穷举搜索（Exhaustive）和非穷举（Non-Exhaustive）两类。</p>
<ul>
<li>广度优先搜索</li>
<li>分支界限搜索</li>
<li>定向搜索</li>
<li>最优有限搜索</li>
<li>启发式搜索<ul>
<li>序列前向选择</li>
<li>序列后向选择</li>
<li>双向搜索</li>
<li>增L去R选择算法</li>
<li>序列浮动选择</li>
<li>决策树</li>
</ul>
</li>
<li>随机搜索<ul>
<li>随机产生序列选择算法</li>
<li>模拟退火算法</li>
<li>遗传算法</li>
</ul>
</li>
</ul>
<h6 id="2-特征评估"><a href="#2-特征评估" class="headerlink" title="2. 特征评估"></a>2. 特征评估</h6><p>评价函数</p>
<ol>
<li>相关性</li>
<li>距离</li>
<li>信息增益</li>
<li>一致性</li>
<li>分类器错误率<h6 id="3-特征验证"><a href="#3-特征验证" class="headerlink" title="3. 特征验证"></a>3. 特征验证</h6><h5 id=""><a href="#" class="headerlink" title=""></a></h5><h5 id="最小角回归-Least-angle-Regression-LARS"><a href="#最小角回归-Least-angle-Regression-LARS" class="headerlink" title="最小角回归(Least-angle Regression, LARS)"></a>最小角回归(Least-angle Regression, LARS)</h5>针对高位数据，最小角回归LARS算法首先是一种逐步向前回归。在逐步向前的每一步中，它都会找到与目标最相关的特征。当特征具有相等的相关性时，它不是沿着相同的特征继续进行，而是沿着特征之间等角的方向进行。</li>
</ol>
<p>参考<a target="_blank" rel="noopener" href="https://www.yuque.com/attachments/yuque/0/2022/pdf/22905381/1647050640766-48899bc1-94c0-4077-acd1-4cc01a2f3f96.pdf?_lake_card=%7B%22src%22:%22https://www.yuque.com/attachments/yuque/0/2022/pdf/22905381/1647050640766-48899bc1-94c0-4077-acd1-4cc01a2f3f96.pdf%22,%22name%22:%22LeastAngle_2002.pdf%22,%22size%22:350701,%22type%22:%22application/pdf%22,%22ext%22:%22pdf%22,%22status%22:%22done%22,%22taskId%22:%22u71c5bfbb-358b-4da0-8f62-c2fdd74f52c%22,%22taskType%22:%22upload%22,%22id%22:%22DQVG6%22,%22card%22:%22file%22%7D">LeastAngle_2002.pdf</a>了解更多最小角回归算法的更多细节。</p>
<h5 id="LARS-Lasso"><a href="#LARS-Lasso" class="headerlink" title="LARS Lasso"></a>LARS Lasso</h5><p>LarsLasso是利用LARS算法实现的LASSO模型，与基于坐标下降的LASSO模型不同，它得到的是分段线性的精确解，是其自身系数范数的函数。<br><img src="https://github.com/moakap/moakap.github.io/assets/6308127/32e0ee89-64ec-4805-9660-4374e5b13ff4" alt="image"></p>
<h5 id="正交匹配追踪（OMP）"><a href="#正交匹配追踪（OMP）" class="headerlink" title="正交匹配追踪（OMP）"></a>正交匹配追踪（OMP）</h5><p>一种类似于最小角回归的前向特征选择方法，正交匹配追踪可以用固定数目的非零元素逼近最优解向量。或者，正交匹配追踪可以针对特定的误差，而不是特定数目的非零系数。</p>
<p>参考 <a target="_blank" rel="noopener" href="https://www.yuque.com/attachments/yuque/0/2022/pdf/22905381/1647053096423-abbbd6a4-e47d-43e5-8311-b82c733eb989.pdf?_lake_card=%7B%22src%22:%22https://www.yuque.com/attachments/yuque/0/2022/pdf/22905381/1647053096423-abbbd6a4-e47d-43e5-8311-b82c733eb989.pdf%22,%22name%22:%22KSVD-OMP-v2.pdf%22,%22size%22:171887,%22type%22:%22application/pdf%22,%22ext%22:%22pdf%22,%22status%22:%22done%22,%22taskId%22:%22ud5d3e5f7-88e3-4f46-b39c-556ad51ae98%22,%22taskType%22:%22upload%22,%22id%22:%22uac6e41f4%22,%22card%22:%22file%22%7D">KSVD-OMP-v2.pdf</a>。</p>
<h5 id="随机梯度下降（SGD）"><a href="#随机梯度下降（SGD）" class="headerlink" title="随机梯度下降（SGD）"></a>随机梯度下降（SGD）</h5><p>随机梯度下降是一种简单而又非常有效的拟合线性模型的方法。当样本数量(和特性数量)非常大时，它特别有用。</p>
<h5 id="感知机-（Perceptron）"><a href="#感知机-（Perceptron）" class="headerlink" title="感知机 （Perceptron）"></a>感知机 （Perceptron）</h5><p><a target="_blank" rel="noopener" href="https://scikit-learn.org.cn/view/384.html">Perceptron</a> 是另一种适用于大规模学习的简单分类算法。有如下默认：</p>
<ul>
<li>它不需要设置学习率</li>
<li>它不需要正则项</li>
<li>它只用错误样本更新模型</li>
</ul>
<p>最后一个特点意味着Perceptron的训练速度略快于带有合页损失(hinge loss)的SGD，<strong>因此得到的模型更稀疏</strong>。</p>
<h5 id="被动感知算法-Passive-Aggressive-Algorithms"><a href="#被动感知算法-Passive-Aggressive-Algorithms" class="headerlink" title="被动感知算法(Passive Aggressive Algorithms)"></a>被动感知算法(Passive Aggressive Algorithms)</h5><p>被动感知算法是一种大规模学习的算法。和感知机相似，因为它们不需要设置学习率。然而，与感知器不同的是，它们包含正则化参数 C 。</p>
<h4 id="广义线性回归（GLM）"><a href="#广义线性回归（GLM）" class="headerlink" title="广义线性回归（GLM）"></a>广义线性回归（GLM）</h4><p>广义线性模型(GLM)以两种方式扩展了线性模型。</p>
<ol>
<li>反向连接函数</li>
</ol>
<p>首先是预测值$\hat y$是否通过反向连接函数$h$连接到输入变量$X$的线性组合。<br>$\hat y(w, X) = h(Xw)$</p>
<ol start="2">
<li>损失函数</li>
</ol>
<p>其次，平方损失函数被一个指数分布的单位偏差$d$所代替 (更准确地说，一个再生指数离散模型(EDM) )。最小化问题变成<br>$\min_w \frac{1}{2n_{samples}}  \sum_i d(y_i, \hat{y_i}) + \frac{\alpha}{2}|w|_2$<br>$\alpha$是L2正则化惩罚项。提供样本权重后，平均值即为加权平均值。</p>
<h5 id="再生指数离散模型-EDM"><a href="#再生指数离散模型-EDM" class="headerlink" title="再生指数离散模型(EDM)"></a>再生指数离散模型(EDM)</h5><p><img src="https://github.com/moakap/moakap.github.io/assets/6308127/e222c2dc-f8bc-4df3-8c71-8ec62cd9452f" alt="image"></p>
<h5 id="TweedieRegressor"><a href="#TweedieRegressor" class="headerlink" title="TweedieRegressor"></a>TweedieRegressor</h5><p>TweedieRegressor为Tweedie分布实现了一个广义线性模型，该模型允许使用适当的$power$参数对上述任何分布进行建模。</p>
<ul>
<li>power = 0: 正态分布。在这种情况下，诸如 <a target="_blank" rel="noopener" href="https://scikit-learn.org.cn/view/399.html">Ridge</a>, <a target="_blank" rel="noopener" href="https://scikit-learn.org.cn/view/404.html">ElasticNet</a> 等特定的估计器通常更合适。</li>
<li>power = 1: 泊松分布。方便起见可以使用 <a target="_blank" rel="noopener" href="https://scikit-learn.org.cn/view/437.html">PoissonRegressor</a> 。然而，它完全等同于 TweedieRegressor(power=1, link=’log’).</li>
<li>power = 2: 伽马分布。方便起见可以使用<a target="_blank" rel="noopener" href="https://scikit-learn.org.cn/view/440.html">GammaRegressor</a> 。然而，它完全等同于 TweedieRegressor(power=2, link=’log’).</li>
<li>power = 3: 逆高斯分布。</li>
</ul>
<h5 id="分配方式选择"><a href="#分配方式选择" class="headerlink" title="分配方式选择"></a>分配方式选择</h5><p>分配方式的选择取决于手头的问题:</p>
<ul>
<li>如果目标值  是计数(非负整数值)或相对频率(非负)，则可以使用带有log-link的泊松偏差。</li>
<li>如果目标值是正的，并且是歪斜的，您可以尝试带有log-link的Gamma偏差。</li>
<li>如果目标值似乎比伽马分布的尾部更重，那么您可以尝试使用逆高斯偏差(或者更高的Tweedie族方差)。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li>【SVR回归分析简明教程】<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33692660">https://zhuanlan.zhihu.com/p/33692660</a></li>
<li>【scikit-learn支持向量机/回归】<a target="_blank" rel="noopener" href="http://scikit-learn.org.cn/view/83.html#">http://scikit-learn.org.cn/view/83.html#</a></li>
<li>【SVR支持向量回归】<a target="_blank" rel="noopener" href="https://scikit-learn.org.cn/view/782.html">https://scikit-learn.org.cn/view/782.html</a></li>
<li>【使用线性与非线性核的支持向量机回归】<a target="_blank" rel="noopener" href="http://scikit-learn.org.cn/view/342.html">http://scikit-learn.org.cn/view/342.html</a></li>
<li>【统计学——回归分析】<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/352694434">https://zhuanlan.zhihu.com/p/352694434</a></li>
<li>【从入门到放弃——线性回归】<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/147297924">https://zhuanlan.zhihu.com/p/147297924</a></li>
<li>【回归模型偏差、方差和残差】<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/50214504">https://zhuanlan.zhihu.com/p/50214504</a></li>
<li>【马氏距离】<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46626607">https://zhuanlan.zhihu.com/p/46626607</a></li>
<li>【Lasso回归和Ridge回归的区别】<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1556213">https://cloud.tencent.com/developer/article/1556213</a></li>
<li>【深入理解L1、L2正则化】<a target="_blank" rel="noopener" href="https://www.cnblogs.com/zingp/p/10375691.html#_label0">https://www.cnblogs.com/zingp/p/10375691.html#_label0</a></li>
<li>【机器学习中正则化项L1和L2的直观理解】<a target="_blank" rel="noopener" href="https://blog.csdn.net/jinping_shi/article/details/52433975">https://blog.csdn.net/jinping_shi/article/details/52433975</a></li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/ml/" rel="tag"><i class="fa fa-tag"></i> ML</a>
              <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> Python</a>
              <a href="/tags/linear-regression/" rel="tag"><i class="fa fa-tag"></i> Linear Regression</a>
              <a href="/tags/sklearn/" rel="tag"><i class="fa fa-tag"></i> Sklearn</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/03/12/ML/ML_basics_and_sklearn_framework/" rel="prev" title="机器学习基础入门和Python sklearn">
      <i class="fa fa-chevron-left"></i> 机器学习基础入门和Python sklearn
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/02/24/superficial/Install_Flarum/" rel="next" title="安装Flarum">
      安装Flarum <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="nav-number">1.</span> <span class="nav-text">机器学习基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-vs-%E4%BC%A0%E7%BB%9F%E7%BC%96%E7%A8%8B"><span class="nav-number">1.1.</span> <span class="nav-text">机器学习 vs. 传统编程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%89%E7%A7%8D%E7%B1%BB%E5%88%AB"><span class="nav-number">1.2.</span> <span class="nav-text">三种类别</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.2.1.</span> <span class="nav-text">监督学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.2.2.</span> <span class="nav-text">非监督学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.2.3.</span> <span class="nav-text">半监督学习</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Python-sklearn-scikit-learn"><span class="nav-number">2.</span> <span class="nav-text">Python sklearn (scikit-learn)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%89%E5%A4%A7%E6%AD%A5%E9%AA%A4"><span class="nav-number">2.1.</span> <span class="nav-text">机器学习三大步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#scikit-learn-sklearn-%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.2.</span> <span class="nav-text">scikit-learn (sklearn) 实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BD%AC%E6%8D%A2%E5%99%A8%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86%E5%99%A8"><span class="nav-number">2.2.1.</span> <span class="nav-text">转换器和预处理器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E5%99%A8"><span class="nav-number">2.2.2.</span> <span class="nav-text">预测器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%A1%E9%81%93"><span class="nav-number">2.2.3.</span> <span class="nav-text">管道</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="nav-number">2.3.</span> <span class="nav-text">模型评估</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E6%8C%87%E6%A0%87"><span class="nav-number">2.3.1.</span> <span class="nav-text">回归算法指标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%8C%87%E6%A0%87"><span class="nav-number">2.3.2.</span> <span class="nav-text">分类算法指标</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E6%90%9C%E7%B4%A2"><span class="nav-number">2.4.</span> <span class="nav-text">参数搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GridSearchCV-%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95"><span class="nav-number">2.4.1.</span> <span class="nav-text">GridSearchCV 网格搜索方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RandomizedSearchCV-%E9%9A%8F%E6%9C%BA%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95"><span class="nav-number">2.4.2.</span> <span class="nav-text">RandomizedSearchCV 随机搜索方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%EF%BC%88%E6%8C%87%E6%A0%87%EF%BC%89"><span class="nav-number">2.4.3.</span> <span class="nav-text">评估方法（指标）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">回归算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%EF%BC%88Regression-Analysis%EF%BC%89"><span class="nav-number">3.1.</span> <span class="nav-text">回归分析（Regression Analysis）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88Liner-Regression%EF%BC%89%E5%9F%BA%E7%A1%80"><span class="nav-number">3.2.</span> <span class="nav-text">线性回归（Liner Regression）基础</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E5%80%BC%E5%92%8C%E7%9C%9F%E5%AE%9E%E5%80%BC%E7%9A%84%E8%B7%9D%E7%A6%BB"><span class="nav-number">3.2.1.</span> <span class="nav-text">预测值和真实值的距离</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E6%AE%8B%E5%B7%AE%E5%B9%B3%E6%96%B9%E5%92%8CSSE"><span class="nav-number">3.2.1.1.</span> <span class="nav-text">1. 残差平方和SSE</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E6%AC%A7%E6%B0%8F%E8%B7%9D%E7%A6%BB"><span class="nav-number">3.2.1.2.</span> <span class="nav-text">2. 欧氏距离</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-%E6%9B%BC%E5%93%88%E9%A1%BF%E8%B7%9D%E7%A6%BB"><span class="nav-number">3.2.1.3.</span> <span class="nav-text">3. 曼哈顿距离</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB-Mahalanobis-Distance"><span class="nav-number">3.2.1.4.</span> <span class="nav-text">4. 马氏距离(Mahalanobis Distance)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-%E5%85%B6%E5%AE%83"><span class="nav-number">3.2.1.5.</span> <span class="nav-text">5. 其它</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%99%AE%E9%80%9A%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95-Ordinary-Least-Square-OLS"><span class="nav-number">3.2.2.</span> <span class="nav-text">普通最小二乘法(Ordinary Least Square, OLS)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9D%9E%E8%B4%9F%E6%9C%80%E5%B0%8F%E6%96%B9%E5%B7%AE"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">非负最小方差</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88-Overfitting-%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88-Underfitting"><span class="nav-number">3.2.3.</span> <span class="nav-text">过拟合(Overfitting)和欠拟合(Underfitting)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">3.2.3.1.</span> <span class="nav-text">正则化</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B2%AD-Ridge-%E5%9B%9E%E5%BD%92%EF%BC%88L2%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%89"><span class="nav-number">3.2.4.</span> <span class="nav-text">岭(Ridge)回归（L2正则化）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Lasso%E5%9B%9E%E5%BD%92%EF%BC%88L1%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%89"><span class="nav-number">3.2.5.</span> <span class="nav-text">Lasso回归（L1正则化）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E9%80%89%E5%8F%96%E4%B8%8E%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5"><span class="nav-number">3.2.5.1.</span> <span class="nav-text">特征选取与压缩感知</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Elastic-Net%E5%BC%B9%E6%80%A7%E7%BD%91%E7%BB%9C%E5%9B%9E%E5%BD%92%EF%BC%88L1-L2%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%89"><span class="nav-number">3.2.6.</span> <span class="nav-text">Elastic-Net弹性网络回归（L1+L2正则化）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E4%BC%B0%E6%AD%A3%E5%88%99%E5%8C%96%E5%8F%82%E6%95%B0%E2%80%94%E2%80%94%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%9B%9E%E5%BD%92"><span class="nav-number">3.2.7.</span> <span class="nav-text">预估正则化参数——贝叶斯回归</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%B2%AD%E5%9B%9E%E5%BD%92"><span class="nav-number">3.2.7.1.</span> <span class="nav-text">贝叶斯岭回归</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E5%85%B3%E8%81%94%E5%88%A4%E5%AE%9A-ARD"><span class="nav-number">3.2.7.2.</span> <span class="nav-text">自动关联判定-ARD</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%AB%98%E7%BB%B4%E6%95%B0%E6%8D%AE%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">3.2.8.</span> <span class="nav-text">高维数据的线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="nav-number">3.2.8.1.</span> <span class="nav-text">特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-%E7%89%B9%E5%BE%81%E7%94%9F%E6%88%90"><span class="nav-number">3.2.8.1.1.</span> <span class="nav-text">1. 特征生成</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-%E7%89%B9%E5%BE%81%E8%AF%84%E4%BC%B0"><span class="nav-number">3.2.8.1.2.</span> <span class="nav-text">2. 特征评估</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-%E7%89%B9%E5%BE%81%E9%AA%8C%E8%AF%81"><span class="nav-number">3.2.8.1.3.</span> <span class="nav-text">3. 特征验证</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-number">3.2.8.2.</span> <span class="nav-text"></span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9C%80%E5%B0%8F%E8%A7%92%E5%9B%9E%E5%BD%92-Least-angle-Regression-LARS"><span class="nav-number">3.2.8.3.</span> <span class="nav-text">最小角回归(Least-angle Regression, LARS)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#LARS-Lasso"><span class="nav-number">3.2.8.4.</span> <span class="nav-text">LARS Lasso</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%AD%A3%E4%BA%A4%E5%8C%B9%E9%85%8D%E8%BF%BD%E8%B8%AA%EF%BC%88OMP%EF%BC%89"><span class="nav-number">3.2.8.5.</span> <span class="nav-text">正交匹配追踪（OMP）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88SGD%EF%BC%89"><span class="nav-number">3.2.8.6.</span> <span class="nav-text">随机梯度下降（SGD）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA-%EF%BC%88Perceptron%EF%BC%89"><span class="nav-number">3.2.8.7.</span> <span class="nav-text">感知机 （Perceptron）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A2%AB%E5%8A%A8%E6%84%9F%E7%9F%A5%E7%AE%97%E6%B3%95-Passive-Aggressive-Algorithms"><span class="nav-number">3.2.8.8.</span> <span class="nav-text">被动感知算法(Passive Aggressive Algorithms)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88GLM%EF%BC%89"><span class="nav-number">3.2.9.</span> <span class="nav-text">广义线性回归（GLM）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%86%8D%E7%94%9F%E6%8C%87%E6%95%B0%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B-EDM"><span class="nav-number">3.2.9.1.</span> <span class="nav-text">再生指数离散模型(EDM)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#TweedieRegressor"><span class="nav-number">3.2.9.2.</span> <span class="nav-text">TweedieRegressor</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%86%E9%85%8D%E6%96%B9%E5%BC%8F%E9%80%89%E6%8B%A9"><span class="nav-number">3.2.9.3.</span> <span class="nav-text">分配方式选择</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">4.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="moakap"
      src="/images/snail-filled.png">
  <p class="site-author-name" itemprop="name">moakap</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">20</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">32</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/moakap" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;moakap" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">moakap</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  













<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'AjpQWK4TAFokhRL5FftLIpHr-gzGzoHsz',
      appKey     : 'Lt5JkPmORcKiBCx0RvGPVcNP',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
